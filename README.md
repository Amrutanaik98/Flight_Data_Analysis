# âœˆï¸ Flight Data Analytics Pipeline

A production-grade, enterprise-scale data pipeline for real-time flight data ingestion, processing, analytics, and visualization. Built with AWS, Apache Airflow, Streamlit, and Machine Learning.

**Status:** ğŸŸ¢ Active Development | Phase 2 Complete | Phase 3 In Progress

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Getting Started](#getting-started)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Phases](#phases)
- [Monitoring](#monitoring)
- [Contributing](#contributing)

---

## ğŸ¯ Overview

This project builds a **complete data pipeline** that:

1. **Ingests** flight data from Aviation Stack API in real-time
2. **Processes** data using AWS Lambda (streaming) and AWS Glue (batch)
3. **Stores** data in multiple AWS services (S3, DynamoDB, SQS)
4. **Analyzes** flight patterns, delays, and performance metrics
5. **Visualizes** insights via Streamlit dashboard
6. **Orchestrates** everything using Apache Airflow on EC2
7. **Alerts** on anomalies and delays in real-time

**Perfect for:** Learning data engineering, portfolio projects, or production-ready systems.

---

## ğŸ—ï¸ Architecture

```
FLIGHT DATA PIPELINE - Data Flow
================================

PHASE 1: DATA INGESTION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aviation API â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   producer.py              â”‚ (Fetches flight data)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ S3     â”‚ â”‚ SQS   â”‚ (Raw data storage)
â”‚ (raw/) â”‚ â”‚ Queue â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜
              â”‚
              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Lambda  â”‚ (Processes messages)
          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
               â”‚
               â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ DynamoDB     â”‚ (Real-time DB)
          â”‚ (flights)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BATCH PROCESSING
    â”‚
    â–¼ (S3 raw/)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AWS Glue   â”‚ (Spark jobs)
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ S3          â”‚
  â”‚(processed/) â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 2: ANALYTICS & VISUALIZATION
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                         â”‚
    â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analytics    â”‚      â”‚ Streamlit        â”‚
â”‚ Engine       â”‚      â”‚ Dashboard        â”‚
â”‚              â”‚      â”‚ Port: 8055       â”‚
â”‚ - Airline    â”‚      â”‚                  â”‚
â”‚   Performanceâ”‚      â”‚ - Real-time KPIs â”‚
â”‚ - Routes     â”‚      â”‚ - Interactive    â”‚
â”‚ - Delays     â”‚      â”‚   Charts         â”‚
â”‚ - Trends     â”‚      â”‚ - Data Tables    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Reportsâ”‚
   â”‚ (S3)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ORCHESTRATION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apache Airflow on EC2                â”‚
â”‚ Port: 8080                           â”‚
â”‚ - Runs daily                         â”‚
â”‚ - Monitors all tasks                 â”‚
â”‚ - Email alerts                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Features

### Phase 1: Data Ingestion âœ…
- âœ… Real-time flight data from Aviation Stack API
- âœ… Dual storage: S3 (raw) + SQS (queue)
- âœ… Lambda-based transformation
- âœ… DynamoDB for real-time queries
- âœ… AWS Glue batch processing

### Phase 2: Analytics & Reporting âœ…
- âœ… Real-time Streamlit dashboard with KPIs
- âœ… Airline performance analysis
- âœ… Route optimization insights
- âœ… Delay pattern analysis
- âœ… Automated daily reports
- âœ… Historical trend analysis

### Phase 3: Advanced Analytics ğŸš§
- ğŸ”„ Machine Learning (delay prediction)
- ğŸ”„ Real-time alerts
- ğŸ”„ REST API
- ğŸ”„ Anomaly detection

### Phase 4: Infrastructure ğŸ“…
- ğŸ“‹ Data warehouse (Redshift/BigQuery)
- ğŸ“‹ Mobile app (React Native/Flutter)
- ğŸ“‹ Web portal (React.js)

### Phase 5: Enterprise ğŸ“…
- ğŸ“‹ Docker containerization
- ğŸ“‹ Kubernetes deployment
- ğŸ“‹ Security & compliance
- ğŸ“‹ Multi-source integration

---

## ğŸ› ï¸ Tech Stack

### Cloud Platform
- **AWS:** EC2, S3, DynamoDB, SQS, Lambda, AWS Glue, CloudWatch, SNS

### Data Processing
- **Apache Airflow** - Workflow orchestration
- **AWS Glue/Spark** - Batch processing
- **Python 3.9+** - All scripts
- **Pandas** - Data manipulation
- **boto3** - AWS SDK

### Frontend & Visualization
- **Streamlit** - Real-time dashboard
- **Plotly** - Interactive charts
- **Pydeck** - Geospatial visualization

### Development & Deployment
- **Git/GitHub** - Version control
- **Terraform** - Infrastructure as Code
- **Docker** - Containerization (Phase 5)
- **Kubernetes** - Orchestration (Phase 5)

---

## ğŸš€ Getting Started

### Prerequisites
- AWS Account with free tier eligibility
- Python 3.9+ installed
- Git installed
- EC2 instance running Ubuntu 22.04 LTS
- SSH access to EC2

### Quick Start (10 minutes)

1. **Clone Repository**
```bash
git clone https://github.com/Amrutanaik98/flight-data-analysis.git
cd flight-data-analysis
```

2. **Create Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install Dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure AWS**
```bash
aws configure
# Enter: AWS Access Key ID, Secret Key, Region (us-east-1)
```

5. **Deploy Infrastructure**
```bash
cd terraform/aws_resources
terraform init
terraform apply
```

6. **Deploy EC2 & Airflow**
```bash
cd ../airflow_ec2
terraform init
terraform apply
```



---

## ğŸ“¦ Installation

### Local Development
```bash
git clone https://github.com/Amrutanaik98/flight-data-analysis.git
cd flight-data-analysis
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
aws configure
```

### Run Locally
```bash
# Run producer
python src/producer.py

# Run analytics
python analytics/flight_analytics.py

# Run dashboard
streamlit run dashboard/dashboard.py --server.port 8055
```

### AWS Deployment
```bash
# Deploy with Terraform
cd terraform/aws_resources
terraform init && terraform apply

cd ../airflow_ec2
terraform init && terraform apply

# SSH to EC2
ssh -i ~/.ssh/airflow-key.pem ubuntu@<PUBLIC_IP>

# Restart Airflow
pkill -f airflow
airflow scheduler > /tmp/scheduler.log 2>&1 &
airflow webserver --port 8080 > /tmp/webserver.log 2>&1 &
```

---

## ğŸ’» Usage

### Run Pipeline

**Option 1: Trigger via CLI**
```bash
airflow dags trigger flight_data_pipeline
```

**Option 2: Trigger via Airflow UI**
1. Open http://34.195.227.103:8080
2. Find flight_data_pipeline
3. Click blue play button

**Option 3: Scheduled (Automatic)**
- Runs daily automatically
- Monitor via Airflow dashboard

### View Analytics

```bash
# Streamlit dashboard
streamlit run dashboard/dashboard.py --server.port 8055

# Run analytics manually
python analytics/flight_analytics.py

# Generate reports
python analytics/generate_reports.py

# View reports in S3
aws s3 ls s3://flights-data-lake-amruta/analytics/reports/
```

---

## ğŸ“ Project Structure

```
flight-data-analysis/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producer.py              # Fetch API data
â”‚   â”œâ”€â”€ producer_debug.py        # Testing
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ flight_analytics.py      # Analytics engine
â”‚   â”œâ”€â”€ generate_reports.py      # Report generation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ reports/                 # Generated reports
â”‚   â””â”€â”€ data/                    # Processed data
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ dashboard.py             # Main Streamlit app
â”‚   â”œâ”€â”€ dashboard_test.py        # Test version
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ run.sh                   # Start script
â”‚   â””â”€â”€ components/              # UI components
â”‚
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ health_monitor.py        # Health checks
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ alert_manager.py         # Alerts
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ flight_data_dag.py  # Main DAG
â”‚   â”œâ”€â”€ logs/                    # Execution logs
â”‚   â””â”€â”€ airflow.cfg              # Config
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ aws_resources/           # AWS setup
â”‚   â””â”€â”€ airflow_ec2/             # EC2 setup
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_producer.py
â”‚   â”œâ”€â”€ test_analytics.py
â”‚   â””â”€â”€ test_api.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â””â”€â”€ SETUP.md
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ Dockerfile
```

---

## ğŸ“Š Data Flow

```
1. Producer fetches API data
   â†“
2. Data â†’ S3 (raw/) + SQS
   â†“
3. Lambda triggered (external)
   â†“
4. DynamoDB updated (real-time)
   â†“
5. Glue job processes S3 (batch)
   â†“
6. S3 processed/ updated
   â†“
7. Analytics engine analyzes
   â†“
8. Reports generated
   â†“
9. Dashboard updates
   â†“
10. Alerts sent (if needed)
```

---

## ğŸ“Š Phases

### âœ… Phase 1: Infrastructure (Complete)
- AWS resources deployed
- Terraform IaC ready
- EC2 with Airflow running

### âœ… Phase 2: Analytics & Reporting (Complete)
- Streamlit dashboard live
- Daily analytics running
- Automated reports generated

### ğŸ”„ Phase 3: Advanced Analytics (In Progress)
- ML delay prediction model
- Real-time alert system
- REST API endpoints

### ğŸ“‹ Phase 4: Data Infrastructure (Planned)
- Data warehouse setup
- Mobile app development
- Web portal

### ğŸ“‹ Phase 5: Enterprise (Planned)
- Docker containerization
- Kubernetes deployment
- Security & compliance

---

## ğŸ“ˆ Monitoring

### Airflow Dashboard
- **URL:** http://34.195.227.103:8080
- **Features:** Task status, logs, scheduling, retries

### Streamlit Dashboard
- **URL:** http://34.195.227.103:8055
- **Features:** Real-time KPIs, charts, tables, trends

### CloudWatch Logs
```bash
# Lambda logs
aws logs tail /aws/lambda/flights-consumer-dev --follow

# Glue logs
aws logs tail /aws-glue/flights-job-dev --follow
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Test producer
python src/producer_debug.py

# Test analytics
python analytics/flight_analytics.py

# Test dashboard
streamlit run dashboard/dashboard_test.py
```

---

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch (`git checkout -b feature/YourFeature`)
3. Commit changes (`git commit -m 'Add YourFeature'`)
4. Push to branch (`git push origin feature/YourFeature`)
5. Open Pull Request

---

## ğŸ“ License

MIT License - See LICENSE file for details

---

## ğŸ‘©â€ğŸ’¼ Author

**Amruta Naik**
- GitHub: [@Amrutanaik98](https://github.com/Amrutanaik98)
- Project: [Flight Data Analysis](https://github.com/Amrutanaik98/flight-data-analysis)

---

## ğŸ’¬ Support

For issues or questions:
1. Check [Documentation](docs/)
2. Search [GitHub Issues](https://github.com/Amrutanaik98/flight-data-analysis/issues)
3. Create new issue with details

---

## ğŸ¯ Next Steps

1. âœ… Phase 2 Analytics
2. ğŸ”„ Phase 3 Machine Learning
3. ğŸ“‹ Phase 4 Data Infrastructure
4. ğŸ³ Phase 5 Docker/Kubernetes

---

## ğŸ“Š Project Statistics

- **Lines of Code:** 3,500+
- **AWS Services:** 8+
- **Phases:** 5 (2 complete, 3 planned)
- **Analytics Metrics:** 20+
- **Dashboard Visualizations:** 15+

---

**Last Updated:** November 26, 2025
**Status:** ğŸŸ¢ Active Development
**Next Phase:** Phase 3 - Machine Learning
