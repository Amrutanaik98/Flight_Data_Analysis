from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import subprocess
import boto3

AWS_REGION = "us-east-1"
S3_BUCKET = "flights-data-lake-amruta"
SQS_QUEUE_NAME = "flight_dev"
DYNAMODB_TABLE = "flights-realtime-dev"
GLUE_JOB_NAME = "flight_data_processing"
PRODUCER_SCRIPT = "/home/ubuntu/Flight_Data_Analysis/src/producer.py"

default_args = {
    'owner': 'airflow',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'flight_data_pipeline',
    default_args=default_args,
    description='Flight Data Pipeline - Producer â†’ S3/SQS â†’ Lambda â†’ DynamoDB, Glue â†’ S3 Processed',
    schedule_interval='*/30 * * * *',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['flights', 'production', 'data-pipeline'],
)

def run_producer_script():
    print("=" * 70)
    print("ğŸš€ STEP 1: RUNNING PRODUCER SCRIPT")
    print("=" * 70)
    print("Action: Fetch flight API data")
    print("Output: Data â†’ S3 raw/ + SQS queue")
    print("")
    
    try:
        result = subprocess.run(
            ['python', PRODUCER_SCRIPT],
            capture_output=True,
            text=True,
            timeout=600,
        )
        
        print("PRODUCER OUTPUT:")
        print(result.stdout)
        
        if result.returncode != 0:
            print("PRODUCER ERROR:")
            print(result.stderr)
            raise Exception(f"Producer failed with code {result.returncode}")
        
        print("=" * 70)
        print("âœ… STEP 1 COMPLETE: Data sent to S3 raw + SQS")
        print("=" * 70)
        return "Producer executed successfully"
        
    except Exception as e:
        print(f"âŒ ERROR: {str(e)}")
        raise

def check_sqs_messages():
    print("=" * 70)
    print("ğŸ” STEP 2: CHECKING SQS QUEUE")
    print("=" * 70)
    print("Action: Verify messages in SQS")
    print("Note: Lambda will pick these up and process them â†’ DynamoDB")
    print("")
    
    try:
        sqs_client = boto3.client('sqs', region_name=AWS_REGION)
        response = sqs_client.get_queue_url(QueueName=SQS_QUEUE_NAME)
        queue_url = response['QueueUrl']
        
        response = sqs_client.get_queue_attributes(
            QueueUrl=queue_url,
            AttributeNames=['ApproximateNumberOfMessages']
        )
        
        msg_count = int(response['Attributes']['ApproximateNumberOfMessages'])
        print(f"ğŸ“¨ Messages in queue: {msg_count}")
        print("=" * 70)
        print("âœ… STEP 2 COMPLETE: SQS checked")
        print("=" * 70)
        return f"SQS: {msg_count} messages"
        
    except Exception as e:
        print(f"âš ï¸ SQS warning: {str(e)}")
        return "SQS skipped"

def check_lambda_processed_dynamodb():
    print("=" * 70)
    print("âš¡ STEP 3: CHECKING LAMBDA â†’ DYNAMODB")
    print("=" * 70)
    print("Action: Verify Lambda processed SQS messages and loaded to DynamoDB")
    print("Note: Lambda runs outside Airflow, triggered by SQS")
    print("")
    
    try:
        dynamodb = boto3.resource('dynamodb', region_name=AWS_REGION)
        table = dynamodb.Table(DYNAMODB_TABLE)
        
        response = table.scan(Limit=10)
        item_count = response['Count']
        
        print(f"ğŸ“Š Items in DynamoDB (Lambda output): {item_count}")
        
        if item_count > 0:
            sample_item = response['Items'][0]
            print(f"ğŸ“‹ Sample item keys: {list(sample_item.keys())}")
            print(f"âœ… Lambda successfully processed SQS â†’ DynamoDB!")
        else:
            print("âš ï¸ No items found - Lambda may not have processed yet")
        
        print("=" * 70)
        print("âœ… STEP 3 COMPLETE: Lambda/DynamoDB checked")
        print("=" * 70)
        return f"DynamoDB: {item_count} items (from Lambda)"
        
    except Exception as e:
        print(f"âš ï¸ DynamoDB warning: {str(e)}")
        return "DynamoDB skipped"

def trigger_glue_job():
    print("=" * 70)
    print("âš™ï¸  STEP 4: TRIGGERING GLUE JOB")
    print("=" * 70)
    print("Action: Start AWS Glue job for batch processing")
    print("Input: S3 raw/ folder")
    print("Output: S3 processed/ folder (aggregated data)")
    print("")
    
    try:
        glue_client = boto3.client('glue', region_name=AWS_REGION)
        
        response = glue_client.start_job_run(
            JobName=GLUE_JOB_NAME,
            Arguments={
                '--input_bucket': S3_BUCKET,
                '--input_prefix': 'raw/',
                '--output_prefix': 'processed/',
            }
        )
        
        job_run_id = response['JobRunId']
        print(f"ğŸš€ Glue job started!")
        print(f"Job Run ID: {job_run_id}")
        print("=" * 70)
        print("âœ… STEP 4 COMPLETE: Glue job triggered")
        print("=" * 70)
        return f"Glue triggered: {job_run_id}"
        
    except Exception as e:
        print(f"âš ï¸ Glue warning: {str(e)}")
        return "Glue skipped"

def wait_for_glue_job():
    print("=" * 70)
    print("â³ STEP 5: MONITORING GLUE JOB")
    print("=" * 70)
    print("Action: Check Glue job status")
    print("")
    
    try:
        glue_client = boto3.client('glue', region_name=AWS_REGION)
        response = glue_client.get_job_runs(JobName=GLUE_JOB_NAME, MaxResults=1)
        
        if response['JobRuns']:
            job_run = response['JobRuns'][0]
            job_state = job_run['JobRunState']
            print(f"Current Glue job state: {job_state}")
            
            if job_state == 'SUCCEEDED':
                print("âœ… Glue job completed successfully!")
            elif job_state == 'FAILED':
                print("âŒ Glue job failed!")
        
        print("=" * 70)
        print("âœ… STEP 5 COMPLETE: Glue monitoring done")
        print("=" * 70)
        
    except Exception as e:
        print(f"âš ï¸ Glue monitoring warning: {str(e)}")

def check_processed_data():
    print("=" * 70)
    print("ğŸ”„ STEP 6: CHECKING PROCESSED DATA")
    print("=" * 70)
    print("Action: Verify Glue job output in S3")
    print("Source: S3 processed/ folder")
    print("")
    
    try:
        s3_client = boto3.client('s3', region_name=AWS_REGION)
        
        response = s3_client.list_objects_v2(
            Bucket=S3_BUCKET,
            Prefix='processed/'
        )
        
        if 'Contents' not in response:
            print("â„¹ï¸ No processed files found yet (Glue may still be running)")
            return "No processed data yet"
        
        file_count = len(response['Contents'])
        print(f"âœ… Found {file_count} files in S3 processed/")
        
        latest_file = max(response['Contents'], key=lambda x: x['LastModified'])
        print(f"ğŸ“„ Latest file: {latest_file['Key']}")
        print(f"ğŸ“Š Size: {latest_file['Size']} bytes")
        
        print("=" * 70)
        print("âœ… STEP 6 COMPLETE: Processed data verified")
        print("=" * 70)
        return f"Processed: {file_count} files"
        
    except Exception as e:
        print(f"âš ï¸ Processed data warning: {str(e)}")
        return "Processed skipped"

def send_notification():
    print("=" * 70)
    print("âœ… PIPELINE EXECUTION COMPLETE")
    print("=" * 70)
    print("")
    print("ğŸ‰ FLIGHT DATA PIPELINE COMPLETED!")
    print("")
    print("ğŸ“‹ Complete Data Flow:")
    print("  1. Producer.py â†’ Fetched API data")
    print("  2. Data â†’ S3 raw/ + SQS queue")
    print("  3. Lambda (external) processed SQS â†’ DynamoDB (real-time)")
    print("  4. Glue job processed S3 raw/ â†’ S3 processed/ (batch)")
    print("  5. All validations passed")
    print("")
    print(f"Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    return "Pipeline complete"

task_start = BashOperator(
    task_id='pipeline_start',
    bash_command='echo "ğŸš€ Flight Data Pipeline Started"',
    dag=dag,
)

task_producer = PythonOperator(
    task_id='step_1_run_producer',
    python_callable=run_producer_script,
    dag=dag,
)

task_check_sqs = PythonOperator(
    task_id='step_2_check_sqs',
    python_callable=check_sqs_messages,
    dag=dag,
)

task_lambda_dynamodb = PythonOperator(
    task_id='step_3_lambda_dynamodb',
    python_callable=check_lambda_processed_dynamodb,
    dag=dag,
)

task_trigger_glue = PythonOperator(
    task_id='step_4_trigger_glue',
    python_callable=trigger_glue_job,
    dag=dag,
)

task_wait_glue = PythonOperator(
    task_id='step_5_wait_glue',
    python_callable=wait_for_glue_job,
    dag=dag,
)

task_check_processed = PythonOperator(
    task_id='step_6_check_processed',
    python_callable=check_processed_data,
    dag=dag,
)

task_notify = PythonOperator(
    task_id='step_7_send_notification',
    python_callable=send_notification,
    dag=dag,
)

task_end = BashOperator(
    task_id='pipeline_complete',
    bash_command='echo "âœ… Flight Data Pipeline Completed"',
    dag=dag,
)

task_start >> task_producer >> task_check_sqs >> task_lambda_dynamodb >> task_trigger_glue >> task_wait_glue >> task_check_processed >> task_notify >> task_end